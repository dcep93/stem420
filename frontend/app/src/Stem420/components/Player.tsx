import { useCallback, useEffect, useMemo, useRef, useState } from "react";

import { TrackRow } from "./player/TrackRow";
import { drawVisualizer } from "./player/visualizers";
import {
  AMPLITUDE_WINDOW_SECONDS,
  type CachedTrackFile,
  type PlayerProps,
  type Track,
  type VisualizerType,
} from "./player/types";

export default function Player({ record, onClose }: PlayerProps) {
  const [isPlaying, setIsPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const [duration, setDuration] = useState(0);
  const [trackDurations, setTrackDurations] = useState<Record<string, number>>({});
  const [volumes, setVolumes] = useState<Record<string, number>>({});
  const [amplitudeEnvelopes, setAmplitudeEnvelopes] = useState<
    Record<string, number[]>
  >({});
  const [amplitudeMaximums, setAmplitudeMaximums] = useState<
    Record<string, number>
  >({});
  const [visualizerType, setVisualizerType] = useState<VisualizerType>("laser-ladders");
  const [trackMuteStates, setTrackMuteStates] = useState<Record<string, boolean>>({});
  const [trackDeafenStates, setTrackDeafenStates] = useState<Record<string, boolean>>({});
  const [readyTrackIds, setReadyTrackIds] = useState<string[]>([]);
  const isAnyTrackDeafened = useMemo(
    () => Object.values(trackDeafenStates).some(Boolean),
    [trackDeafenStates]
  );

  const audioCtxRef = useRef<AudioContext | null>(null);
  const buffersRef = useRef<Record<string, AudioBuffer>>({});
  const gainNodesRef = useRef<Record<string, GainNode>>({});
  const analyserNodesRef = useRef<Record<string, AnalyserNode>>({});
  const sourcesRef = useRef<Record<string, AudioBufferSourceNode>>({});
  const canvasRefs = useRef<Record<string, HTMLCanvasElement | null>>({});
  const drawAnimationFrameRef = useRef<number | null>(null);
  const timeAnimationFrameRef = useRef<number | null>(null);
  const isDraggingSeekRef = useRef(false);
  const pendingSeekRef = useRef<number | null>(null);
  const startAtCtxTimeRef = useRef(0);
  const startOffsetRef = useRef(0);

  const tracks = useMemo<Track[]>(() => {
    return record.files
      .filter(
        (file: CachedTrackFile) => !file.name.toLowerCase().endsWith(".json")
      )
      .map((file: CachedTrackFile, index: number) => ({
        id: `${record.md5}-${index}`,
        name: file.name,
        path: file.path,
        isInput: file.path.includes("/input/"),
        url: URL.createObjectURL(file.blob),
        blob: file.blob,
      }));
  }, [record]);

  const primaryTrack = tracks.find((track) => track.isInput) ?? tracks[0];
  const playerTitle = primaryTrack?.name ?? "Playback";

  const trackLookup = useMemo(() => {
    return tracks.reduce<Record<string, Track>>((lookup, track) => {
      lookup[track.id] = track;
      return lookup;
    }, {});
  }, [tracks]);

  const getEffectiveVolume = useCallback(
    (trackId: string, baseVolume?: number) => {
      const track = trackLookup[trackId];
      const volume = baseVolume ?? volumes[trackId] ?? 1;

      if (!track) {
        return volume;
      }

      const isTrackMuted = trackMuteStates[trackId];
      const isTrackDeafened = trackDeafenStates[trackId];

      if (isTrackMuted) {
        return 0;
      }

      if (isAnyTrackDeafened && !isTrackDeafened) {
        return 0;
      }

      return volume;
    },
    [isAnyTrackDeafened, trackDeafenStates, trackLookup, trackMuteStates, volumes]
  );

  const ensureAudioContext = useCallback(() => {
    if (!audioCtxRef.current) {
      audioCtxRef.current = new AudioContext();
    }

    return audioCtxRef.current;
  }, []);

  const stopAllSources = useCallback(() => {
    Object.values(sourcesRef.current).forEach((source) => {
      try {
        source.stop();
      } catch (error) {
        console.warn("Failed to stop source", error);
      }
      source.disconnect();
    });

    sourcesRef.current = {};
  }, []);

  const applyEffectiveVolume = useCallback(
    (trackId: string, baseVolume?: number) => {
      const context = audioCtxRef.current ?? ensureAudioContext();
      const gainNode = gainNodesRef.current[trackId];

      if (!context || !gainNode) {
        return;
      }

      const targetVolume = getEffectiveVolume(trackId, baseVolume);
      const now = context.currentTime;

      gainNode.gain.setTargetAtTime(targetVolume, now, 0.01);
    },
    [ensureAudioContext, getEffectiveVolume]
  );

  const currentPlaybackTime = useCallback(() => {
    const context = audioCtxRef.current;

    if (!context) {
      return startOffsetRef.current;
    }

    if (isPlaying) {
      return (
        context.currentTime - startAtCtxTimeRef.current + startOffsetRef.current
      );
    }

    return startOffsetRef.current;
  }, [isPlaying]);

  useEffect(() => {
    const audioContextSnapshot = audioCtxRef.current;

    return () => {
      if (drawAnimationFrameRef.current !== null) {
        cancelAnimationFrame(drawAnimationFrameRef.current);
      }

      if (timeAnimationFrameRef.current !== null) {
        cancelAnimationFrame(timeAnimationFrameRef.current);
      }

      stopAllSources();
      audioContextSnapshot?.close().catch((error) => {
        console.error("Failed to close audio context", error);
      });
    };
  }, [stopAllSources]);

  useEffect(() => {
    /* eslint-disable react-hooks/set-state-in-effect */
    const initialVolumes: Record<string, number> = {};

    for (const track of tracks) {
      initialVolumes[track.id] = 1;
    }

    setVolumes(initialVolumes);
    setCurrentTime(0);
    setDuration(0);
    setTrackDurations({});
    setIsPlaying(false);
    setReadyTrackIds([]);
    setTrackMuteStates({});
    setTrackDeafenStates({});
    setAmplitudeEnvelopes({});
    setAmplitudeMaximums({});
    startOffsetRef.current = 0;
    startAtCtxTimeRef.current = 0;
    stopAllSources();
    buffersRef.current = {};
    gainNodesRef.current = {};
    analyserNodesRef.current = {};

    const tracksSnapshot = tracks;

    return () => {
      tracksSnapshot.forEach((track) => {
        URL.revokeObjectURL(track.url);
      });
    };
    /* eslint-enable react-hooks/set-state-in-effect */
  }, [stopAllSources, tracks]);

  useEffect(() => {
    const activeIds = new Set(tracks.map((track) => track.id));

    Object.entries(gainNodesRef.current).forEach(([id, gainNode]) => {
      if (!activeIds.has(id)) {
        gainNode.disconnect();
        delete gainNodesRef.current[id];
      }
    });

    Object.entries(analyserNodesRef.current).forEach(([id, analyser]) => {
      if (!activeIds.has(id)) {
        analyser.disconnect();
        delete analyserNodesRef.current[id];
      }
    });

    Object.keys(buffersRef.current).forEach((id) => {
      if (!activeIds.has(id)) {
        delete buffersRef.current[id];
      }
    });
  }, [tracks]);

  useEffect(() => {
    let isCancelled = false;
    const context = ensureAudioContext();

    const analyzeTrack = async (track: Track) => {
      try {
        const audioBuffer = await context.decodeAudioData(
          (await track.blob.arrayBuffer()).slice(0)
        );

        if (isCancelled) {
          return;
        }

        buffersRef.current[track.id] = audioBuffer;
        const gain = context.createGain();
        const analyser = context.createAnalyser();

        analyser.fftSize = 2048;
        gain.connect(analyser);
        analyser.connect(context.destination);

        gainNodesRef.current[track.id] = gain;
        analyserNodesRef.current[track.id] = analyser;
        applyEffectiveVolume(track.id, volumes[track.id]);

        setTrackDurations((previous) => {
          const next = { ...previous, [track.id]: audioBuffer.duration };
          const durations = Object.values(next);
          const maxDuration = durations.length ? Math.max(...durations) : 0;
          setDuration(Number.isFinite(maxDuration) ? maxDuration : 0);
          return next;
        });

        const windowSize = Math.max(
          1,
          Math.floor(audioBuffer.sampleRate * AMPLITUDE_WINDOW_SECONDS)
        );
        const envelope: number[] = [];
        const channelCount = audioBuffer.numberOfChannels;
        const totalWindows = Math.ceil(audioBuffer.length / windowSize);

        for (let windowIndex = 0; windowIndex < totalWindows; windowIndex++) {
          let sumSquares = 0;
          const start = windowIndex * windowSize;
          const end = Math.min(start + windowSize, audioBuffer.length);

          for (let channel = 0; channel < channelCount; channel++) {
            const channelData = audioBuffer.getChannelData(channel);
            for (let i = start; i < end; i++) {
              sumSquares += channelData[i]! * channelData[i]!;
            }
          }

          const sampleCount = (end - start) * channelCount;
          const rms = sampleCount > 0 ? Math.sqrt(sumSquares / sampleCount) : 0;
          envelope.push(rms);
        }

        if (isCancelled) {
          return;
        }

        const peak = envelope.reduce((max, value) => Math.max(max, value), 0);
        setAmplitudeMaximums((previous) => ({
          ...previous,
          [track.id]: peak > 0 ? peak : 1,
        }));
        setAmplitudeEnvelopes((previous) => ({
          ...previous,
          [track.id]: envelope,
        }));
        setReadyTrackIds((previous) => {
          if (previous.includes(track.id)) {
            return previous;
          }

          return [...previous, track.id];
        });
      } catch (error) {
        console.error("Failed to analyze track envelope", track.name, error);
      }
    };

    tracks.forEach((track) => {
      void analyzeTrack(track);
    });

    return () => {
      isCancelled = true;
    };
  }, [applyEffectiveVolume, ensureAudioContext, tracks, volumes]);

  useEffect(() => {
    tracks.forEach((track) => {
      applyEffectiveVolume(track.id);
    });
  }, [applyEffectiveVolume, getEffectiveVolume, tracks]);

  useEffect(() => {
    const resizeCanvases = () => {
      Object.values(canvasRefs.current).forEach((canvas) => {
        if (!canvas) {
          return;
        }

        const parentWidth = canvas.parentElement?.clientWidth ?? window.innerWidth;
        const nextWidth = Math.max(0, Math.floor(parentWidth));

        if (nextWidth && canvas.width !== nextWidth) {
          canvas.width = nextWidth;
        }
      });
    };

    resizeCanvases();
    window.addEventListener("resize", resizeCanvases);

    return () => {
      window.removeEventListener("resize", resizeCanvases);
    };
  }, [tracks]);

  useEffect(() => {
    const draw = () => {
      const sampleRate = audioCtxRef.current?.sampleRate ?? 44100;
      const playbackTime = currentPlaybackTime();

      tracks.forEach((track) => {
        const analyser = analyserNodesRef.current[track.id];
        const canvas = canvasRefs.current[track.id];

        if (!analyser || !canvas) {
          return;
        }

        try {
          drawVisualizer({
            analyser,
            canvas,
            visualizerType,
            amplitudeEnvelope: amplitudeEnvelopes[track.id],
            amplitudeMaximum: amplitudeMaximums[track.id],
            sampleRate,
            currentTime: playbackTime,
            duration: duration || 0,
          });
        } catch (error) {
          console.error("Failed to draw visualizer", track.name, error);
        }
      });

      drawAnimationFrameRef.current = requestAnimationFrame(draw);
    };

    drawAnimationFrameRef.current = requestAnimationFrame(draw);

    return () => {
      if (drawAnimationFrameRef.current !== null) {
        cancelAnimationFrame(drawAnimationFrameRef.current);
      }
    };
  }, [
    amplitudeEnvelopes,
    amplitudeMaximums,
    currentPlaybackTime,
    duration,
    tracks,
    visualizerType,
  ]);

  useEffect(() => {
    const updateTime = () => {
      const playbackTime = currentPlaybackTime();
      if (isDraggingSeekRef.current) {
        const pendingTime = pendingSeekRef.current;

        if (pendingTime !== null) {
          setCurrentTime(Math.min(duration || pendingTime, pendingTime));
        }
      } else {
        setCurrentTime(Math.min(duration || playbackTime, playbackTime));
      }

      if (isPlaying && duration && playbackTime >= duration) {
        stopAllSources();
        startOffsetRef.current = duration;
        setIsPlaying(false);
      }

      timeAnimationFrameRef.current = requestAnimationFrame(updateTime);
    };

    timeAnimationFrameRef.current = requestAnimationFrame(updateTime);

    return () => {
      if (timeAnimationFrameRef.current !== null) {
        cancelAnimationFrame(timeAnimationFrameRef.current);
      }
    };
  }, [currentPlaybackTime, duration, isPlaying, stopAllSources]);

  const schedulePlayback = useCallback(
    async (offsetSeconds: number) => {
      if (!tracks.length) {
        return;
      }

      const readyTracks = tracks.filter(
        (track) => buffersRef.current[track.id] && gainNodesRef.current[track.id]
      );

      if (!readyTracks.length) {
        return;
      }

      const context = ensureAudioContext();
      await context.resume();
      const startAt = context.currentTime + 0.02;
      const newSources: Record<string, AudioBufferSourceNode> = {};

      readyTracks.forEach((track) => {
        const buffer = buffersRef.current[track.id];
        const gainNode = gainNodesRef.current[track.id];

        if (!buffer || !gainNode) {
          return;
        }

        const source = context.createBufferSource();
        source.buffer = buffer;
        source.connect(gainNode);
        source.start(startAt, offsetSeconds);
        newSources[track.id] = source;
      });

      sourcesRef.current = newSources;
      startAtCtxTimeRef.current = startAt;
      setIsPlaying(true);
    },
    [ensureAudioContext, tracks]
  );

  const commitSeek = useCallback(
    async (targetTime: number) => {
      const clampedTime = Math.max(0, Math.min(targetTime, duration || 0));

      if (isPlaying) {
        stopAllSources();
        startOffsetRef.current = clampedTime;
        await schedulePlayback(clampedTime);
      } else {
        startOffsetRef.current = clampedTime;
        setCurrentTime(clampedTime);
      }
    },
    [duration, isPlaying, schedulePlayback, stopAllSources]
  );

  const handleSeekChange = (event: React.ChangeEvent<HTMLInputElement>) => {
    const newTime = Number(event.target.value);
    setCurrentTime(newTime);
    pendingSeekRef.current = newTime;

    if (!isDraggingSeekRef.current) {
      void commitSeek(newTime);
    }
  };

  const handleSeekStart = () => {
    isDraggingSeekRef.current = true;
  };

  const handleSeekEnd = () => {
    isDraggingSeekRef.current = false;
    const pending = pendingSeekRef.current;

    if (pending !== null) {
      void commitSeek(pending);
    }

    pendingSeekRef.current = null;
  };

  const handlePlayPause = async () => {
    if (!tracks.length) {
      return;
    }

    if (isPlaying) {
      const context = audioCtxRef.current;

      if (!context) {
        setIsPlaying(false);
        return;
      }

      startOffsetRef.current =
        context.currentTime - startAtCtxTimeRef.current + startOffsetRef.current;
      stopAllSources();
      setIsPlaying(false);
      return;
    }

    await schedulePlayback(startOffsetRef.current);
  };

  const handleVolumeChange = (trackId: string, value: number) => {
    setVolumes((previous) => ({ ...previous, [trackId]: value }));
    applyEffectiveVolume(trackId, value);
  };

  const toggleTrackMute = (trackId: string) => {
    setTrackMuteStates((previous) => ({
      ...previous,
      [trackId]: !previous[trackId],
    }));
    applyEffectiveVolume(trackId);
  };

  const toggleTrackDeafen = (trackId: string) => {
    setTrackDeafenStates((previous) => ({
      ...previous,
      [trackId]: !previous[trackId],
    }));
    applyEffectiveVolume(trackId);
  };

  const formattedTime = (time: number) => {
    const safeTime = Math.max(0, Math.floor(time));
    const minutes = Math.floor(safeTime / 60)
      .toString()
      .padStart(2, "0");
    const seconds = (safeTime % 60).toString().padStart(2, "0");
    return `${minutes}:${seconds}`;
  };

  const areTracksReady = tracks.length > 0 && readyTrackIds.length === tracks.length;

  if (!tracks.length) {
    return null;
  }

  return (
    <div
      style={{ marginTop: "1.5rem", padding: "1rem", border: "1px solid #444" }}
    >
      <div
        style={{
          display: "flex",
          justifyContent: "space-between",
          alignItems: "center",
        }}
      >
        <h3 style={{ margin: 0 }}>{playerTitle}</h3>
        <button type="button" onClick={onClose}>
          Close
        </button>
      </div>
      <div
        style={{
          marginTop: "0.75rem",
          display: "flex",
          alignItems: "center",
          gap: "0.5rem",
          flexWrap: "wrap",
        }}
      >
        <input
          type="range"
          min={0}
          max={duration || 0}
          step={0.1}
          value={Math.min(currentTime, duration || 0)}
          onChange={handleSeekChange}
          onPointerDown={handleSeekStart}
          onPointerUp={handleSeekEnd}
          onPointerCancel={handleSeekEnd}
          style={{
            width: "60%",
            minWidth: "240px",
            verticalAlign: "middle",
          }}
        />
        <span style={{ marginRight: "0.5rem" }}>
          {formattedTime(currentTime)} / {formattedTime(duration)}
        </span>
      </div>
      <div style={{ marginTop: "0.5rem" }}>
        <button
          type="button"
          onClick={() => void handlePlayPause()}
          disabled={!areTracksReady}
        >
          {isPlaying ? "Pause" : areTracksReady ? "Play" : "Loading..."}
        </button>
      </div>
      <div style={{ marginTop: "0.75rem", display: "flex", gap: "0.5rem" }}>
        <label htmlFor="visualizer-type" style={{ fontWeight: 600 }}>
          Visualizer
        </label>
        <select
          id="visualizer-type"
          value={visualizerType}
          onChange={(event) =>
            setVisualizerType(event.target.value as VisualizerType)
          }
        >
          <option value="laser-ladders">Laser Ladders (Graphic EQ)</option>
          <option value="spectrum-safari">Spectrum Safari (Analyzer)</option>
          <option value="waveform-waterline">
            Waveform Waterline (Oscilloscope)
          </option>
          <option value="aurora-radar">Aurora Radar (Radial Sweep)</option>
          <option value="mirror-peaks">Mirror Peaks (Symmetric Bars)</option>
          <option value="pulse-grid">Pulse Grid (Energy Matrix)</option>
          <option value="luminous-orbit">Luminous Orbit (Layered Rings)</option>
          <option value="nebula-trails">Nebula Trails (Shimmering Path)</option>
          <option value="time-ribbon">Time Ribbon (Amplitude Timeline)</option>
        </select>
      </div>
      <div style={{ marginTop: "1rem" }}>
        {tracks.map((track) => {
          const trackDuration = trackDurations[track.id];
          const durationLabel = Number.isFinite(trackDuration)
            ? `${trackDuration.toFixed(4)}s`
            : "Loading duration...";

          return (
            <TrackRow
              key={track.id}
              track={track}
              durationLabel={durationLabel}
              volume={volumes[track.id] ?? 1}
              isMuted={!!trackMuteStates[track.id]}
              isDeafened={!!trackDeafenStates[track.id]}
              onVolumeChange={handleVolumeChange}
              onToggleMute={toggleTrackMute}
              onToggleDeafen={toggleTrackDeafen}
              registerCanvas={(ref) => {
                canvasRefs.current[track.id] = ref;
              }}
            />
          );
        })}
      </div>
    </div>
  );
}
