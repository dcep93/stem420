import {
  type CSSProperties,
  type PointerEvent,
  useCallback,
  useEffect,
  useMemo,
  useRef,
  useState,
} from "react";

import {
  cacheChordTimeline,
  getCachedChordTimeline,
  removeCachedOutputs,
} from "../indexedDbClient";
import {
  applyAudioEffect,
  audioEffectOptions,
  type AudioEffectType,
  createEffectNodes,
  type EffectNodes,
  getDefaultEffectValue,
} from "./player/audioEffects";
import {
  analyzeChordTimeline,
  type ChordSnapshot,
} from "./player/chordAnalyzer";
import { TrackRow } from "./player/TrackRow";
import {
  AMPLITUDE_WINDOW_SECONDS,
  type CachedTrackFile,
  FUTURE_WINDOW_SECONDS,
  PAST_WINDOW_SECONDS,
  type PlayerProps,
  type Track,
  type VisualizerType,
} from "./player/types";
import { drawVisualizer } from "./player/visualizers";

const visualizerOptions: Array<{
  value: VisualizerType;
  label: string;
  hint: string;
}> = [
  { value: "time-ribbon", label: "Time Ribbon", hint: "Amplitude Timeline" },
  { value: "laser-ladders", label: "Laser Ladders", hint: "Graphic EQ" },
  { value: "spectrum-safari", label: "Spectrum Safari", hint: "Analyzer" },
  {
    value: "waveform-waterline",
    label: "Waveform Waterline",
    hint: "Oscilloscope",
  },
  { value: "aurora-radar", label: "Aurora Radar", hint: "Radial Sweep" },
  { value: "mirror-peaks", label: "Mirror Peaks", hint: "Symmetric Bars" },
  { value: "pulse-grid", label: "Pulse Grid", hint: "Energy Matrix" },
  { value: "luminous-orbit", label: "Luminous Orbit", hint: "Layered Rings" },
  { value: "prism-bloom", label: "Prism Bloom", hint: "Radiant Arcs" },
  {
    value: "cascade-horizon",
    label: "Cascade Horizon",
    hint: "Layered Terrain",
  },
  { value: "nebula-trails", label: "Nebula Trails", hint: "Shimmering Path" },
  { value: "echo-lantern", label: "Echo Lantern", hint: "Glowing Ripples" },
  { value: "ember-mandala", label: "Ember Mandala", hint: "Radiant Petals" },
  { value: "hippie-mirage", label: "Hippie Mirage", hint: "Tie-Dye Bloom" },
  { value: "hollow-echoes", label: "Hollow Echoes", hint: "Stacked Pillars" },
  { value: "opal-current", label: "Opal Current", hint: "Opalescent Waves" },
  { value: "solstice-waves", label: "Solstice Waves", hint: "Solar Horizon" },
  { value: "ripple-weave", label: "Ripple Weave", hint: "Braided Ribbons" },
  { value: "ectoplasm", label: "Ectoplasm", hint: "Plasma Bloom" },
  {
    value: "super-time-ribbon",
    label: "Super Time Ribbon",
    hint: "Shaking Ribbon",
  },
  {
    value: "prismatic-turbine",
    label: "Prismatic Turbine",
    hint: "Whirling Shards",
  },
  { value: "kaleidoscope", label: "Kaleidoscope", hint: "Mirrored Lenses" },
  { value: "highway", label: "Highway", hint: "Retro Neon Run" },
  { value: "delay-pedal", label: "Delay Pedal", hint: "Echo Ripples" },
];

export default function Player({ record, onClose }: PlayerProps) {
  const [isPlaying, setIsPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const [duration, setDuration] = useState(0);
  const [volumes, setVolumes] = useState<Record<string, number>>({});
  const [amplitudeEnvelopes, setAmplitudeEnvelopes] = useState<
    Record<string, number[]>
  >({});
  const [amplitudeMaximums, setAmplitudeMaximums] = useState<
    Record<string, number>
  >({});
  const [visualizerType, setVisualizerType] =
    useState<VisualizerType>("time-ribbon");
  const [trackMuteStates, setTrackMuteStates] = useState<
    Record<string, boolean>
  >({});
  const [trackDeafenStates, setTrackDeafenStates] = useState<
    Record<string, boolean>
  >({});
  const [effectValues, setEffectValues] = useState<Record<string, number>>({});
  const [effectTypes, setEffectTypes] = useState<
    Record<string, AudioEffectType>
  >({});
  const [readyTrackIds, setReadyTrackIds] = useState<string[]>([]);
  const [isClearingCache, setIsClearingCache] = useState(false);
  const [chordTimeline, setChordTimeline] = useState<ChordSnapshot[]>([]);
  const [chordStatus, setChordStatus] = useState<string>(
    "Analyzing harmony..."
  );
  const [currentChord, setCurrentChord] = useState<string>("Detecting...");
  const isAnyTrackDeafened = useMemo(
    () => Object.values(trackDeafenStates).some(Boolean),
    [trackDeafenStates]
  );

  const volumesRef = useRef<Record<string, number>>({});
  const trackMuteStatesRef = useRef<Record<string, boolean>>({});
  const trackDeafenStatesRef = useRef<Record<string, boolean>>({});
  const effectValuesRef = useRef<Record<string, number>>({});
  const effectTypesRef = useRef<Record<string, AudioEffectType>>({});

  const audioCtxRef = useRef<AudioContext | null>(null);
  const buffersRef = useRef<Record<string, AudioBuffer>>({});
  const gainNodesRef = useRef<Record<string, GainNode>>({});
  const effectNodesRef = useRef<Record<string, EffectNodes>>({});
  const analyserNodesRef = useRef<Record<string, AnalyserNode>>({});
  const sourcesRef = useRef<Record<string, AudioBufferSourceNode>>({});
  const canvasRefs = useRef<Record<string, HTMLCanvasElement | null>>({});
  const drawAnimationFrameRef = useRef<number | null>(null);
  const timeAnimationFrameRef = useRef<number | null>(null);
  const isDraggingSeekRef = useRef(false);
  const pendingSeekRef = useRef<number | null>(null);
  const startAtCtxTimeRef = useRef(0);
  const startOffsetRef = useRef(0);

  const tracks = useMemo<Track[]>(() => {
    return record.files
      .filter(
        (file: CachedTrackFile) => !file.name.toLowerCase().endsWith(".json")
      )
      .map((file: CachedTrackFile, index: number) => ({
        id: `${record.md5}-${index}`,
        name: file.name,
        path: file.path,
        isInput: file.path.includes("/input/"),
        url: URL.createObjectURL(file.blob),
        blob: file.blob,
      }))
      .sort((a, b) => {
        if (a.isInput === b.isInput) {
          return a.name.localeCompare(b.name);
        }

        return a.isInput ? 1 : -1;
      });
  }, [record]);

  const inputTrack = useMemo(
    () =>
      tracks.find(
        (track) => track.isInput && track.name.toLowerCase().endsWith(".mp3")
      ) ??
      tracks.find((track) => track.isInput) ??
      null,
    [tracks]
  );
  const inputTrackId = inputTrack?.id ?? null;

  const primaryTrack = tracks.find((track) => track.isInput) ?? tracks[0];
  const playerTitle = primaryTrack?.name ?? "Playback";
  const chordDisplay = chordTimeline.length
    ? currentChord
    : chordStatus ?? "Analyzing harmony...";

  const trackLookup = useMemo(() => {
    return tracks.reduce<Record<string, Track>>((lookup, track) => {
      lookup[track.id] = track;
      return lookup;
    }, {});
  }, [tracks]);

  const visualizerButtonStyle = useCallback(
    (isActive: boolean): CSSProperties => ({
      borderRadius: "14px",
      border: isActive ? "1px solid #6ddcff" : "1px solid #1f2a3d",
      background: isActive
        ? "linear-gradient(135deg, rgba(37,99,235,0.9), rgba(14,165,233,0.8))"
        : "linear-gradient(135deg, rgba(17,24,39,0.85), rgba(15,23,42,0.9))",
      color: "#e5e7eb",
      padding: "0.65rem 0.85rem",
      minWidth: "200px",
      textAlign: "left",
      boxShadow: isActive
        ? "0 0 0 1px rgba(109,220,255,0.35), 0 16px 40px rgba(0,0,0,0.45)"
        : "0 10px 28px rgba(0,0,0,0.35)",
      cursor: "pointer",
      transition: "all 160ms ease",
    }),
    []
  );

  useEffect(() => {
    volumesRef.current = volumes;
  }, [volumes]);

  useEffect(() => {
    trackMuteStatesRef.current = trackMuteStates;
  }, [trackMuteStates]);

  useEffect(() => {
    trackDeafenStatesRef.current = trackDeafenStates;
  }, [trackDeafenStates]);

  useEffect(() => {
    effectValuesRef.current = effectValues;
  }, [effectValues]);

  useEffect(() => {
    effectTypesRef.current = effectTypes;
  }, [effectTypes]);

  const getEffectiveVolumeFromRefs = useCallback(
    (trackId: string, baseVolume?: number) => {
      const track = trackLookup[trackId];
      const volume = baseVolume ?? volumesRef.current[trackId] ?? 1;

      if (!track) {
        return volume;
      }

      const isTrackMuted = trackMuteStatesRef.current[trackId];
      const isTrackDeafened = trackDeafenStatesRef.current[trackId];
      const isAnyDeafened = Object.values(trackDeafenStatesRef.current).some(
        Boolean
      );

      if (isTrackMuted) {
        return 0;
      }

      if (isAnyDeafened && !isTrackDeafened) {
        return 0;
      }

      return volume;
    },
    [trackLookup]
  );

  const getEffectiveVolume = useCallback(
    (trackId: string, baseVolume?: number) => {
      const track = trackLookup[trackId];
      const volume = baseVolume ?? volumes[trackId] ?? 1;

      if (!track) {
        return volume;
      }

      const isTrackMuted = trackMuteStates[trackId];
      const isTrackDeafened = trackDeafenStates[trackId];

      if (isTrackMuted) {
        return 0;
      }

      if (isAnyTrackDeafened && !isTrackDeafened) {
        return 0;
      }

      return volume;
    },
    [
      isAnyTrackDeafened,
      trackDeafenStates,
      trackLookup,
      trackMuteStates,
      volumes,
    ]
  );

  const ensureAudioContext = useCallback(() => {
    if (!audioCtxRef.current) {
      audioCtxRef.current = new AudioContext();
    }

    return audioCtxRef.current;
  }, []);

  const stopAllSources = useCallback(() => {
    Object.values(sourcesRef.current).forEach((source) => {
      try {
        source.stop();
      } catch (error) {
        console.warn("Failed to stop source", error);
      }
      source.disconnect();
    });

    sourcesRef.current = {};
  }, []);

  const applyEffectiveVolume = useCallback(
    (trackId: string, baseVolume?: number) => {
      const context = audioCtxRef.current ?? ensureAudioContext();
      const gainNode = gainNodesRef.current[trackId];

      if (!context || !gainNode) {
        return;
      }

      const targetVolume = getEffectiveVolume(trackId, baseVolume);
      const now = context.currentTime;

      gainNode.gain.setTargetAtTime(targetVolume, now, 0.01);
    },
    [ensureAudioContext, getEffectiveVolume]
  );

  const applyEffectValue = useCallback(
    (trackId: string, value?: number, effectOverride?: AudioEffectType) => {
      const context = audioCtxRef.current ?? ensureAudioContext();
      const effectNodes = effectNodesRef.current[trackId];

      if (!context || !effectNodes) {
        return;
      }

      const effectType =
        effectOverride ?? effectTypesRef.current[trackId] ?? "wah";
      const effectValue =
        value ??
        effectValuesRef.current[trackId] ??
        getDefaultEffectValue(effectType);

      applyAudioEffect({
        context,
        nodes: effectNodes,
        effect: effectType,
        value: effectValue,
      });
    },
    [ensureAudioContext]
  );

  const currentPlaybackTime = useCallback(() => {
    const context = audioCtxRef.current;

    if (!context) {
      return startOffsetRef.current;
    }

    if (isPlaying) {
      return (
        context.currentTime - startAtCtxTimeRef.current + startOffsetRef.current
      );
    }

    return startOffsetRef.current;
  }, [isPlaying]);

  useEffect(() => {
    const audioContextSnapshot = audioCtxRef.current;

    return () => {
      if (drawAnimationFrameRef.current !== null) {
        cancelAnimationFrame(drawAnimationFrameRef.current);
      }

      if (timeAnimationFrameRef.current !== null) {
        cancelAnimationFrame(timeAnimationFrameRef.current);
      }

      stopAllSources();
      audioContextSnapshot?.close().catch((error) => {
        console.error("Failed to close audio context", error);
      });
    };
  }, [stopAllSources]);

  useEffect(() => {
    const initialVolumes: Record<string, number> = {};
    const initialMuteStates: Record<string, boolean> = {};
    const initialDeafenStates: Record<string, boolean> = {};
    const initialEffectValues: Record<string, number> = {};
    const initialEffectTypes: Record<string, AudioEffectType> = {};

    for (const track of tracks) {
      initialVolumes[track.id] = 1;
      initialMuteStates[track.id] = track.isInput;
      initialDeafenStates[track.id] = false;
      initialEffectValues[track.id] = getDefaultEffectValue("wah");
      initialEffectTypes[track.id] = "wah";
    }

    volumesRef.current = initialVolumes;
    trackMuteStatesRef.current = initialMuteStates;
    trackDeafenStatesRef.current = initialDeafenStates;
    effectValuesRef.current = initialEffectValues;
    effectTypesRef.current = initialEffectTypes;

    setVolumes(initialVolumes);
    setEffectValues(initialEffectValues);
    setEffectTypes(initialEffectTypes);
    setCurrentTime(0);
    setDuration(0);
    setIsPlaying(false);
    setReadyTrackIds([]);
    setTrackMuteStates(initialMuteStates);
    setTrackDeafenStates(initialDeafenStates);
    setAmplitudeEnvelopes({});
    setAmplitudeMaximums({});
    startOffsetRef.current = 0;
    startAtCtxTimeRef.current = 0;
    stopAllSources();
    buffersRef.current = {};
    gainNodesRef.current = {};
    effectNodesRef.current = {};
    analyserNodesRef.current = {};

    const tracksSnapshot = tracks;

    return () => {
      tracksSnapshot.forEach((track) => {
        URL.revokeObjectURL(track.url);
      });
    };
  }, [stopAllSources, tracks]);

  useEffect(() => {
    const activeIds = new Set(tracks.map((track) => track.id));

    Object.entries(gainNodesRef.current).forEach(([id, gainNode]) => {
      if (!activeIds.has(id)) {
        gainNode.disconnect();
        delete gainNodesRef.current[id];
      }
    });

    Object.entries(effectNodesRef.current).forEach(([id, effectNodes]) => {
      if (!activeIds.has(id)) {
        effectNodes.filter.disconnect();
        effectNodes.wetGain.disconnect();
        effectNodes.dryGain.disconnect();
        effectNodes.delay.disconnect();
        effectNodes.feedbackGain.disconnect();
        effectNodes.shaper.disconnect();
        effectNodes.convolver.disconnect();
        effectNodes.pitchShifter.disconnect();
        effectNodes.delayLfoGain.disconnect();
        effectNodes.filterLfoGain.disconnect();
        try {
          effectNodes.lfo.stop();
        } catch (error) {
          console.warn("Failed to stop LFO", error);
        }
        effectNodes.lfo.disconnect();
        delete effectNodesRef.current[id];
      }
    });

    Object.entries(analyserNodesRef.current).forEach(([id, analyser]) => {
      if (!activeIds.has(id)) {
        analyser.disconnect();
        delete analyserNodesRef.current[id];
      }
    });

    Object.keys(buffersRef.current).forEach((id) => {
      if (!activeIds.has(id)) {
        delete buffersRef.current[id];
      }
    });
  }, [tracks]);

  useEffect(() => {
    let isCancelled = false;
    const context = ensureAudioContext();
    const activeTrackIds = new Set(tracks.map((track) => track.id));

    const analyzeTrack = async (track: Track) => {
      try {
        const audioBuffer = await context.decodeAudioData(
          (await track.blob.arrayBuffer()).slice(0)
        );

        if (isCancelled || !activeTrackIds.has(track.id)) {
          return;
        }

        buffersRef.current[track.id] = audioBuffer;
        const gain = context.createGain();
        const effectNodes = createEffectNodes(context);
        const analyser = context.createAnalyser();

        analyser.fftSize = 2048;

        gain.connect(effectNodes.filter);
        gain.connect(effectNodes.dryGain);
        effectNodes.filter.connect(effectNodes.delay);
        effectNodes.wetGain.connect(analyser);
        effectNodes.dryGain.connect(analyser);
        analyser.connect(context.destination);

        gainNodesRef.current[track.id] = gain;
        effectNodesRef.current[track.id] = effectNodes;
        analyserNodesRef.current[track.id] = analyser;
        gain.gain.setValueAtTime(
          getEffectiveVolumeFromRefs(track.id, volumesRef.current[track.id]),
          context.currentTime
        );
        const startingEffect =
          effectTypesRef.current[track.id] ?? ("wah" as AudioEffectType);
        const startingValue =
          effectValuesRef.current[track.id] ??
          getDefaultEffectValue(startingEffect);

        applyEffectValue(track.id, startingValue, startingEffect);

        if (isCancelled || !activeTrackIds.has(track.id)) {
          return;
        }

        setEffectValues((previous) => ({
          ...previous,
          [track.id]:
            previous[track.id] ?? getDefaultEffectValue(startingEffect),
        }));
        setEffectTypes((previous) => ({
          ...previous,
          [track.id]: previous[track.id] ?? startingEffect,
        }));

        if (isCancelled || !activeTrackIds.has(track.id)) {
          return;
        }

        setDuration((previous) => {
          const maxDuration = Math.max(previous, audioBuffer.duration);
          return Number.isFinite(maxDuration) ? maxDuration : 0;
        });

        const windowSize = Math.max(
          1,
          Math.floor(audioBuffer.sampleRate * AMPLITUDE_WINDOW_SECONDS)
        );
        const envelope: number[] = [];
        const channelCount = audioBuffer.numberOfChannels;
        const totalWindows = Math.ceil(audioBuffer.length / windowSize);

        for (let windowIndex = 0; windowIndex < totalWindows; windowIndex++) {
          let sumSquares = 0;
          const start = windowIndex * windowSize;
          const end = Math.min(start + windowSize, audioBuffer.length);

          for (let channel = 0; channel < channelCount; channel++) {
            const channelData = audioBuffer.getChannelData(channel);
            for (let i = start; i < end; i++) {
              sumSquares += channelData[i]! * channelData[i]!;
            }
          }

          const sampleCount = (end - start) * channelCount;
          const rms = sampleCount > 0 ? Math.sqrt(sumSquares / sampleCount) : 0;
          envelope.push(rms);
        }

        if (isCancelled || !activeTrackIds.has(track.id)) {
          return;
        }

        const peak = envelope.reduce((max, value) => Math.max(max, value), 0);
        setAmplitudeMaximums((previous) => ({
          ...previous,
          [track.id]: peak > 0 ? peak : 1,
        }));
        setAmplitudeEnvelopes((previous) => ({
          ...previous,
          [track.id]: envelope,
        }));
        setReadyTrackIds((previous) => {
          if (previous.includes(track.id)) {
            return previous;
          }

          return [...previous, track.id];
        });

        if (track.id === inputTrackId) {
          setChordStatus("Checking cached harmony...");

          try {
            const cachedRecord = await getCachedChordTimeline(record.md5);

            if (!isCancelled && cachedRecord) {
              const cachedTimeline = cachedRecord.timeline ?? [];
              setChordTimeline(cachedTimeline);
              setChordStatus(
                cachedTimeline.length
                  ? "Harmonic map ready"
                  : "No obvious chords detected"
              );
              return;
            }
          } catch (cacheError) {
            console.warn("Failed to load cached chord timeline", cacheError);
          }

          setChordStatus("Analyzing harmony from input MP3...");

          const runChordAnalysis = async () => {
            if (isCancelled) {
              return;
            }

            try {
              const timeline = await analyzeChordTimeline(audioBuffer, {
                stableFrameCount: 2,
                minimumConfidence: 0.16,
                windowSeconds: 1.6,
                hopSeconds: 0.5,
                targetSampleRate: 11025,
                yieldEveryFrames: 6,
              });

              if (!isCancelled) {
                setChordTimeline(timeline);
                setChordStatus(
                  timeline.length
                    ? "Harmonic map ready"
                    : "No obvious chords detected"
                );
                try {
                  await cacheChordTimeline(record.md5, timeline);
                } catch (cacheError) {
                  console.warn("Failed to cache chord timeline", cacheError);
                }
              }
            } catch (chordError) {
              console.error("Failed to analyze chord timeline", chordError);
              if (!isCancelled) {
                setChordStatus("Unable to analyze chords for this input");
              }
            }
          };

          const idleCallback = (
            window as Window & {
              requestIdleCallback?: (
                callback: IdleRequestCallback,
                options?: IdleRequestOptions
              ) => number;
            }
          ).requestIdleCallback;

          if (idleCallback) {
            idleCallback(
              () => {
                void runChordAnalysis();
              },
              { timeout: 1000 }
            );
          } else {
            setTimeout(() => {
              void runChordAnalysis();
            }, 0);
          }
        }
      } catch (error) {
        console.error("Failed to analyze track envelope", track.name, error);
      }
    };

    void (async () => {
      await Promise.all(tracks.map((track) => analyzeTrack(track)));
    })();

    return () => {
      isCancelled = true;
    };
  }, [
    applyEffectValue,
    ensureAudioContext,
    getEffectiveVolumeFromRefs,
    inputTrackId,
    record.md5,
    tracks,
  ]);

  useEffect(() => {
    tracks.forEach((track) => {
      applyEffectiveVolume(track.id);
    });
  }, [applyEffectiveVolume, getEffectiveVolume, tracks]);

  useEffect(() => {
    const resizeCanvases = () => {
      Object.values(canvasRefs.current).forEach((canvas) => {
        if (!canvas) {
          return;
        }

        const parentWidth =
          canvas.parentElement?.clientWidth ?? window.innerWidth;
        const nextWidth = Math.max(0, Math.floor(parentWidth));

        if (nextWidth && canvas.width !== nextWidth) {
          canvas.width = nextWidth;
        }
      });
    };

    resizeCanvases();
    window.addEventListener("resize", resizeCanvases);

    return () => {
      window.removeEventListener("resize", resizeCanvases);
    };
  }, [tracks]);

  useEffect(() => {
    const draw = () => {
      const sampleRate = audioCtxRef.current?.sampleRate ?? 44100;
      const playbackTime = currentPlaybackTime();

      tracks.forEach((track) => {
        const analyser = analyserNodesRef.current[track.id];
        const canvas = canvasRefs.current[track.id];

        if (!analyser || !canvas) {
          return;
        }

        try {
          drawVisualizer({
            analyser,
            canvas,
            visualizerType,
            amplitudeEnvelope: amplitudeEnvelopes[track.id],
            amplitudeMaximum: amplitudeMaximums[track.id],
            sampleRate,
            currentTime: playbackTime,
            duration: duration || 0,
          });
        } catch (error) {
          console.error("Failed to draw visualizer", track.name, error);
        }
      });

      drawAnimationFrameRef.current = requestAnimationFrame(draw);
    };

    drawAnimationFrameRef.current = requestAnimationFrame(draw);

    return () => {
      if (drawAnimationFrameRef.current !== null) {
        cancelAnimationFrame(drawAnimationFrameRef.current);
      }
    };
  }, [
    amplitudeEnvelopes,
    amplitudeMaximums,
    currentPlaybackTime,
    duration,
    tracks,
    visualizerType,
  ]);

  useEffect(() => {
    const updateTime = () => {
      const playbackTime = currentPlaybackTime();
      if (isDraggingSeekRef.current) {
        const pendingTime = pendingSeekRef.current;

        if (pendingTime !== null) {
          setCurrentTime(Math.min(duration || pendingTime, pendingTime));
        }
      } else {
        setCurrentTime(Math.min(duration || playbackTime, playbackTime));
      }

      if (isPlaying && duration && playbackTime >= duration) {
        stopAllSources();
        startOffsetRef.current = duration;
        setIsPlaying(false);
      }

      timeAnimationFrameRef.current = requestAnimationFrame(updateTime);
    };

    timeAnimationFrameRef.current = requestAnimationFrame(updateTime);

    return () => {
      if (timeAnimationFrameRef.current !== null) {
        cancelAnimationFrame(timeAnimationFrameRef.current);
      }
    };
  }, [currentPlaybackTime, duration, isPlaying, stopAllSources]);

  useEffect(() => {
    // Keep the displayed chord in sync with the transport position.
    if (!chordTimeline.length) {
      setCurrentChord(chordStatus ?? "Analyzing harmony...");
      return;
    }

    let activeChord = chordTimeline[0]?.chord ?? "Unclear";

    for (let i = 0; i < chordTimeline.length; i++) {
      const snapshot = chordTimeline[i];

      if (!snapshot) {
        continue;
      }

      if (snapshot.time <= currentTime) {
        activeChord = snapshot.chord;
      } else {
        break;
      }
    }

    setCurrentChord(activeChord);
  }, [chordStatus, chordTimeline, currentTime]);

  const schedulePlayback = useCallback(
    async (offsetSeconds: number) => {
      if (!tracks.length) {
        return;
      }

      const readyTracks = tracks.filter(
        (track) =>
          buffersRef.current[track.id] && gainNodesRef.current[track.id]
      );

      if (!readyTracks.length) {
        return;
      }

      const context = ensureAudioContext();
      await context.resume();
      const startAt = context.currentTime + 0.02;
      const newSources: Record<string, AudioBufferSourceNode> = {};

      readyTracks.forEach((track) => {
        const buffer = buffersRef.current[track.id];
        const gainNode = gainNodesRef.current[track.id];

        if (!buffer || !gainNode) {
          return;
        }

        const source = context.createBufferSource();
        source.buffer = buffer;
        source.connect(gainNode);
        source.start(startAt, offsetSeconds);
        newSources[track.id] = source;
      });

      sourcesRef.current = newSources;
      startAtCtxTimeRef.current = startAt;
      setIsPlaying(true);
    },
    [ensureAudioContext, tracks]
  );

  const commitSeek = useCallback(
    async (targetTime: number) => {
      const clampedTime = Math.max(0, Math.min(targetTime, duration || 0));

      if (isPlaying) {
        stopAllSources();
        startOffsetRef.current = clampedTime;
        await schedulePlayback(clampedTime);
      } else {
        startOffsetRef.current = clampedTime;
        setCurrentTime(clampedTime);
      }
    },
    [duration, isPlaying, schedulePlayback, stopAllSources]
  );

  const handleSeekChange = (event: React.ChangeEvent<HTMLInputElement>) => {
    const newTime = Number(event.target.value);
    setCurrentTime(newTime);
    pendingSeekRef.current = newTime;

    if (!isDraggingSeekRef.current) {
      void commitSeek(newTime);
    }
  };

  const handleSeekStart = () => {
    isDraggingSeekRef.current = true;
  };

  const handleSeekEnd = () => {
    isDraggingSeekRef.current = false;
    const pending = pendingSeekRef.current;

    if (pending !== null) {
      void commitSeek(pending);
    }

    pendingSeekRef.current = null;
  };

  const handleCanvasSeek = useCallback(
    (event: PointerEvent<HTMLCanvasElement>) => {
      if (
        visualizerType !== "time-ribbon" &&
        visualizerType !== "super-time-ribbon"
      ) {
        return;
      }

      if (!duration) {
        return;
      }

      const rect = event.currentTarget.getBoundingClientRect();
      const position = rect.width
        ? (event.clientX - rect.left) / rect.width
        : 0;
      const clamped = Math.min(1, Math.max(0, position));
      const totalWindowSeconds = PAST_WINDOW_SECONDS + FUTURE_WINDOW_SECONDS;
      const timeOffset = clamped * totalWindowSeconds - PAST_WINDOW_SECONDS;
      const targetTime = currentTime + timeOffset;
      pendingSeekRef.current = targetTime;
      void commitSeek(targetTime);
    },
    [commitSeek, currentTime, duration, visualizerType]
  );

  const handlePlayPause = useCallback(async () => {
    if (!tracks.length) {
      return;
    }

    if (isPlaying) {
      const context = audioCtxRef.current;

      if (!context) {
        setIsPlaying(false);
        return;
      }

      startOffsetRef.current =
        context.currentTime -
        startAtCtxTimeRef.current +
        startOffsetRef.current;
      stopAllSources();
      setIsPlaying(false);
      return;
    }

    await schedulePlayback(startOffsetRef.current);
  }, [isPlaying, schedulePlayback, stopAllSources, tracks.length]);

  useEffect(() => {
    const handleKeyDown = (event: KeyboardEvent) => {
      if (event.code === "Space" || event.key === " ") {
        event.preventDefault();
        void handlePlayPause();
        return;
      }

      if (event.code === "ArrowRight" || event.key === "ArrowRight") {
        event.preventDefault();
        setVisualizerType((current) => {
          const currentIndex = visualizerOptions.findIndex(
            (option) => option.value === current
          );

          if (currentIndex === -1) {
            return visualizerOptions[0]?.value ?? "time-ribbon";
          }

          return visualizerOptions[
            (currentIndex + 1) % visualizerOptions.length
          ]?.value;
        });
        return;
      }

      if (event.code === "ArrowLeft" || event.key === "ArrowLeft") {
        event.preventDefault();
        setVisualizerType((current) => {
          const currentIndex = visualizerOptions.findIndex(
            (option) => option.value === current
          );

          if (currentIndex === -1) {
            return visualizerOptions[0]?.value ?? "time-ribbon";
          }

          return visualizerOptions[
            (currentIndex - 1 + visualizerOptions.length) %
              visualizerOptions.length
          ]?.value;
        });
      }
    };

    window.addEventListener("keydown", handleKeyDown);

    return () => {
      window.removeEventListener("keydown", handleKeyDown);
    };
  }, [handlePlayPause]);

  const handleVolumeChange = (trackId: string, value: number) => {
    setVolumes((previous) => ({ ...previous, [trackId]: value }));
    applyEffectiveVolume(trackId, value);
  };

  const handleEffectValueChange = (trackId: string, value: number) => {
    const clamped = Math.min(1, Math.max(0, value));

    setEffectValues((previous) => ({ ...previous, [trackId]: clamped }));
    applyEffectValue(trackId, clamped);
  };

  const handleEffectTypeChange = (
    trackId: string,
    effectType: AudioEffectType
  ) => {
    const defaultValue = getDefaultEffectValue(effectType);
    effectValuesRef.current[trackId] = defaultValue;
    setEffectValues((previous) => ({ ...previous, [trackId]: defaultValue }));
    setEffectTypes((previous) => ({ ...previous, [trackId]: effectType }));
    applyEffectValue(trackId, defaultValue, effectType);
  };

  const handleEffectReset = (trackId: string) => {
    const defaultValue = getDefaultEffectValue(
      effectTypesRef.current[trackId] ?? "wah"
    );
    effectValuesRef.current[trackId] = defaultValue;
    setEffectValues((previous) => ({
      ...previous,
      [trackId]: defaultValue,
    }));
    applyEffectValue(trackId, defaultValue);
  };

  const toggleTrackMute = (trackId: string) => {
    setTrackMuteStates((previous) => ({
      ...previous,
      [trackId]: !previous[trackId],
    }));
    applyEffectiveVolume(trackId);
  };

  const toggleTrackDeafen = (trackId: string) => {
    setTrackDeafenStates((previous) => {
      const nextValue = !previous[trackId];

      if (nextValue) {
        setTrackMuteStates((mutePrevious) => ({
          ...mutePrevious,
          [trackId]: false,
        }));
      }

      return {
        ...previous,
        [trackId]: nextValue,
      };
    });
    applyEffectiveVolume(trackId);
  };

  const formattedTime = (time: number) => {
    const safeTime = Math.max(0, Math.floor(time));
    const minutes = Math.floor(safeTime / 60)
      .toString()
      .padStart(2, "0");
    const seconds = (safeTime % 60).toString().padStart(2, "0");
    return `${minutes}:${seconds}`;
  };

  const areTracksReady =
    tracks.length > 0 && readyTrackIds.length === tracks.length;

  const handleClearCache = useCallback(async () => {
    setIsClearingCache(true);

    try {
      await removeCachedOutputs(record.md5);
      onClose();
    } catch (clearError) {
      console.error("Failed to clear cache for record", clearError);
    } finally {
      setIsClearingCache(false);
    }
  }, [onClose, record.md5]);

  useEffect(() => {
    // Reset chord-related UI when the primary track changes.
    setChordTimeline([]);
    setChordStatus(
      inputTrackId
        ? "Analyzing harmony..."
        : "No input MP3 available for analysis"
    );
    setCurrentChord(inputTrackId ? "Detecting..." : "No input MP3 available");
  }, [inputTrackId]);

  if (!tracks.length) {
    return null;
  }

  return (
    <div
      style={{ marginTop: "1.5rem", padding: "1rem", border: "1px solid #444" }}
    >
      <div
        style={{
          display: "flex",
          alignItems: "center",
          gap: "0.75rem",
          flexWrap: "wrap",
          justifyContent: "space-between",
        }}
      >
        <div
          style={{
            display: "flex",
            alignItems: "center",
            gap: "0.75rem",
            flexWrap: "wrap",
            flex: 1,
          }}
        >
          <div style={{ display: "flex", alignItems: "center", gap: "0.5rem" }}>
            <h3 style={{ margin: 0 }}>{playerTitle}</h3>
            <button
              type="button"
              onClick={() => void handleClearCache()}
              disabled={isClearingCache}
            >
              {isClearingCache ? "Clearing..." : "Clear cache"}
            </button>
          </div>
          <div
            style={{
              display: "flex",
              alignItems: "center",
              gap: "0.5rem",
              flex: 1,
              minWidth: "260px",
            }}
          >
            <input
              type="range"
              min={0}
              max={duration || 0}
              step={0.1}
              value={Math.min(currentTime, duration || 0)}
              onChange={handleSeekChange}
              onPointerDown={handleSeekStart}
              onPointerUp={handleSeekEnd}
              onPointerCancel={handleSeekEnd}
              style={{ width: "100%", verticalAlign: "middle" }}
            />
            <span style={{ whiteSpace: "nowrap", minWidth: "120px" }}>
              {formattedTime(currentTime)} / {formattedTime(duration)}
            </span>
          </div>
          <div
            style={{
              display: "flex",
              alignItems: "center",
              gap: "0.35rem",
              minWidth: "200px",
              color: "#cbd5e1",
            }}
            aria-label="Detected chord"
          >
            <span style={{ fontWeight: 700, color: "#e5e7eb" }}>Chord:</span>
            <span
              style={{ fontStyle: chordTimeline.length ? "normal" : "italic" }}
            >
              {chordDisplay}
            </span>
          </div>
          <button
            type="button"
            onClick={() => void handlePlayPause()}
            disabled={!areTracksReady}
            aria-label={isPlaying ? "Pause" : "Play"}
            style={{ minWidth: "3rem" }}
          >
            <span
              style={{
                display: "inline-block",
                width: "1.5em",
                textAlign: "center",
              }}
              aria-hidden
            >
              {isPlaying ? "⏸" : areTracksReady ? "▶" : "⏳"}
            </span>
          </button>
        </div>
        <button type="button" onClick={onClose}>
          Close
        </button>
      </div>
      <div style={{ marginTop: "1rem" }}>
        {tracks.map((track) => (
          <TrackRow
            key={track.id}
            track={track}
            volume={volumes[track.id] ?? 1}
            isMuted={!!trackMuteStates[track.id]}
            isDeafened={!!trackDeafenStates[track.id]}
            effectType={effectTypes[track.id] ?? "wah"}
            effectValue={
              effectValues[track.id] ??
              getDefaultEffectValue(effectTypes[track.id] ?? "wah")
            }
            effectOptions={audioEffectOptions}
            onVolumeChange={handleVolumeChange}
            onEffectValueChange={handleEffectValueChange}
            onEffectTypeChange={handleEffectTypeChange}
            onResetEffect={handleEffectReset}
            onToggleMute={toggleTrackMute}
            onToggleDeafen={toggleTrackDeafen}
            registerCanvas={(ref) => {
              canvasRefs.current[track.id] = ref;
            }}
            onCanvasSeek={handleCanvasSeek}
          />
        ))}
      </div>
      <div style={{ marginTop: "0.75rem" }}>
        <div
          style={{
            display: "flex",
            alignItems: "baseline",
            gap: "0.5rem",
            marginBottom: "0.5rem",
          }}
        >
          <span style={{ fontWeight: 700, color: "#e5e7eb" }}>Visualizer</span>
        </div>
        <div style={{ display: "flex", flexWrap: "wrap", gap: "0.6rem" }}>
          {visualizerOptions.map((option) => {
            const isActive = visualizerType === option.value;

            return (
              <button
                key={option.value}
                type="button"
                onClick={() => setVisualizerType(option.value)}
                style={visualizerButtonStyle(isActive)}
                aria-pressed={isActive}
              >
                <div style={{ fontWeight: 700, letterSpacing: "0.02em" }}>
                  {option.label}
                </div>
                <div
                  style={{
                    color: isActive ? "#eaf2ff" : "#c7d2fe",
                    fontSize: "0.9rem",
                    marginTop: "0.1rem",
                  }}
                >
                  {option.hint}
                </div>
              </button>
            );
          })}
        </div>
      </div>
    </div>
  );
}
